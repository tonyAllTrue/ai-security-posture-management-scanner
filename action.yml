name: 'AI Security Posture Management Scanner'
description: 'Run comprehensive LLM endpoint pentesting and model scanning with automated vulnerability detection and GitHub issue creation'
author: 'AllTrue Inc. (https://alltrue.ai)'

branding:
  icon: 'shield'
  color: 'blue'

inputs:
  # Authentication
  alltrue-api-key:
    description: 'AllTrue API key for authentication'
    required: true
  alltrue-api-url:
    description: 'AllTrue API URL endpoint'
    required: true
  alltrue-customer-id:
    description: 'AllTrue customer ID'
    required: true
  alltrue-organization-id:
    description: 'AllTrue organization ID (UUID)'
    required: false
    default: ''
  alltrue-organization-name:
    description: 'AllTrue organization name (will be resolved to ID at runtime)'
    required: false
    default: ''

  python-version:
    description: 'Python version to use'
    required: false
    default: '3.11'

  # ---------- Execution toggles ----------
  enable-llm-pentest:
    description: 'Enable LLM endpoint pentesting'
    required: false
    default: 'true'
  enable-model-scanning:
    description: 'Enable model scanning'
    required: false
    default: 'false'

  # ---------- Inventory scope ----------
  inventory-scope:
    description: 'Inventory scope: organization|project|resource'
    required: false
    default: 'organization'
  project-ids:
    description: 'Comma-separated project IDs (UUIDs) for project scope'
    required: false
    default: ''
  project-names:
    description: 'Comma-separated project names (will be resolved to IDs at runtime)'
    required: false
    default: ''
  target-resource-ids:
    description: 'Comma-separated resource IDs (for resource scope)'
    required: false
    default: ''
  target-resource-names:
    description: 'Comma-separated resource patterns (supports: substring, repo:org/name, file:name, =exact, *wildcard*)'
    required: false
    default: ''

  # ---------- LLM Pentest parameters ----------
  pentest-template:
    description: 'Pentest template name to use'
    required: false
    default: 'Prompt Injection'
  pentest-num-attempts:
    description: 'Number of attempts per test case to account for LLM response variability (1-5 recommended)'
    required: false
    default: '1'
  pentest-model-mapping:
    description: 'Map resource types to specific models (format: ResourceType1:model1,ResourceType2:model2)'
    required: false
    default: ''
  pentest-apply-guardrails:
    description: 'Apply guardrails during pentest execution'
    required: false
    default: 'false'
  pentest-system-prompt-enabled:
    description: 'Enable system prompt configuration before pentesting'
    required: false
    default: 'false'
  pentest-system-prompt-text:
    description: 'Custom system prompt text to configure on LLM endpoints'
    required: false
    default: ''
  pentest-cleanup-system-prompt:
    description: 'Clear system prompt from resource after pentest completes'
    required: false
    default: 'true'
  pentest-dataset-enabled:
    description: 'Enable dataset configuration for capture-replay pentesting'
    required: false
    default: 'false'
  pentest-dataset-id:
    description: 'Dataset UUID for capture-replay pentesting'
    required: false
    default: ''
  pentest-dataset-name:
    description: 'Dataset name for capture-replay pentesting (will be resolved to UUID)'
    required: false
    default: ''
  pentest-cleanup-dataset:
    description: 'Clear dataset from resource after pentest completes'
    required: false
    default: 'true'
  pentest-resource-system-description-enabled:
    description: 'Enable configuring a resource system description on the endpoint before testing'
    required: false
    default: 'false'
  pentest-resource-system-description-text:
    description: 'Resource system description text to configure on the endpoint before testing'
    required: false
    default: ''
  pentest-cleanup-resource-system-description:
    description: 'Clear the configured resource system description from the endpoint after testing'
    required: false
    default: 'false'

  # ---------- Model scanning parameters ----------
  model-scan-policies:
    description: 'Comma-separated model scan policies'
    required: false
    default: 'model-scan-code-execution-prohibited,model-scan-input-output-operations-prohibited,model-scan-network-access-prohibited,model-scan-malware-signatures-prohibited,model-custom-layers-prohibited'
  model-scan-description:
    description: 'Description for model scans'
    required: false
    default: 'CI Model Scan'

  # ---------- HuggingFace Model Onboarding ----------
  huggingface-onboarding-enabled:
    description: 'Enable HuggingFace model onboarding before scanning'
    required: false
    default: 'false'
  huggingface-models-to-onboard:
    description: 'HuggingFace models to onboard (format: org/repo or org/repo@revision or JSON array)'
    required: false
    default: ''
  huggingface-onboarding-project-id:
    description: 'Project ID to associate onboarded models with (optional, uses first project from PROJECT_IDS/PROJECT_NAMES if not specified)'
    required: false
    default: ''
  huggingface-onboarding-project-name:
    description: 'Project name to associate onboarded models with (preferred). If set, will be resolved to ID at runtime; takes precedence over huggingface-onboarding-project-id.'
    required: false
    default: ''
  huggingface-onboarding-wait-secs:
    description: 'Wait time in seconds after onboarding before scanning (allows backend indexing)'
    required: false
    default: '10'
  huggingface-onboarding-only:
    description: 'Skip inventory selection and scan only onboarded HuggingFace models'
    required: false
    default: 'false'

  # ---------- Failure thresholds ----------
  fail-outcome-at-or-above:
    description: 'Fail on outcomes at/above this level: critical|poor|moderate|good'
    required: false
    default: 'moderate'
  on-threshold-action:
    description: 'Action on threshold breach: fail|issue|both|none'
    required: false
    default: 'fail'
  on-hard-failures-action:
    description: 'Action on hard failures: fail|issue|both|ignore'
    required: false
    default: 'ignore'

  # ---------- GitHub Issues integration ----------
  github-token:
    description: 'GitHub token for creating issues'
    required: false
    default: ''
  github-repository:
    description: 'Target GitHub repository for issues (owner/repo)'
    required: false
    default: ''
  github-default-labels:
    description: 'Comma-separated default labels for GitHub issues'
    required: false
    default: ''
  github-assignees:
    description: 'Comma-separated GitHub usernames to assign issues'
    required: false
    default: ''
  category-issue-min-severity:
    description: 'Minimum severity for per-category/per-policy issues: CRITICAL|HIGH|MEDIUM|LOW|INFORMATIONAL|none (none disables per-category/per-policy issues)'
    required: false
    default: 'INFORMATIONAL'

  # ---------- Concurrency & polling ----------
  max-concurrent-pentests:
    description: 'Maximum number of pentests/scans to run in parallel'
    required: false
    default: '8'
  start-stagger-secs:
    description: 'Seconds to stagger between start requests'
    required: false
    default: '0'
  max-start-retries:
    description: 'Retries when starting a pentest fails (retryable)'
    required: false
    default: '3'
  start-retry-delay:
    description: 'Seconds to wait between start retries'
    required: false
    default: '30'
  poll-timeout-secs:
    description: 'Timeout in seconds for polling operations'
    required: false
    default: '5400'
  poll-timeout-action:
    description: 'What to do on poll timeout: fail|continue|partial'
    required: false
    default: 'fail'
  graphql-poll-interval-secs:
    description: 'Interval (seconds) between GraphQL polls for execution completion'
    required: false
    default: '30'

  # ---------- Misc ----------
  artifact-retention-days:
    description: 'Days to retain artifacts'
    required: false
    default: '30'

outputs:
  overall-status:
    description: 'Overall scan status (success|failure|neutral)'
    value: ${{ steps.decide.outputs.overall_status }}
  llm-pentest-status:
    description: 'LLM pentest status (success|failure|neutral)'
    value: ${{ steps.decide.outputs.llm_pentest_status }}
  model-scan-status:
    description: 'Model scan status (success|failure|neutral)'
    value: ${{ steps.decide.outputs.model_scan_status }}
  worst-outcome:
    description: 'Worst outcome from all scans (Critical|Poor|Moderate|Good|Excellent|Unknown)'
    value: ${{ steps.extract-outcome.outputs.overall_outcome }}

runs:
  using: 'composite'
  steps:
    - name: Checkout caller repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ inputs.python-version }}

    - name: Install dependencies
      shell: bash
      run: |
        python -m pip install --upgrade pip
        python -m pip install -r "${{ github.action_path }}/requirements.txt"

    # Run the scanner *from the caller workspace* so JSON/CSVs land somewhere the workflow can see.
    - name: Run security scan script
      id: scan
      continue-on-error: true
      shell: bash
      working-directory: ${{ github.workspace }}
      env:
        API_KEY: ${{ inputs.alltrue-api-key }}
        API_URL: ${{ inputs.alltrue-api-url }}
        CUSTOMER_ID: ${{ inputs.alltrue-customer-id }}
        ORGANIZATION_ID: ${{ inputs.alltrue-organization-id }}
        ORGANIZATION_NAME: ${{ inputs.alltrue-organization-name }}

        # Execution toggles
        ENABLE_LLM_PENTEST: ${{ inputs.enable-llm-pentest }}
        ENABLE_MODEL_SCANNING: ${{ inputs.enable-model-scanning }}

        # Inventory
        INVENTORY_SCOPE: ${{ inputs.inventory-scope }}
        PROJECT_IDS: ${{ inputs.project-ids }}
        PROJECT_NAMES: ${{ inputs.project-names }}
        TARGET_RESOURCE_IDS: ${{ inputs.target-resource-ids }}
        TARGET_RESOURCE_NAMES: ${{ inputs.target-resource-names }}

        # LLM Pentest
        TARGET_TEMPLATE_NAME: ${{ inputs.pentest-template }}
        PENTEST_NUM_ATTEMPTS: ${{ inputs.pentest-num-attempts }}
        PENTEST_MODEL_MAPPING: ${{ inputs.pentest-model-mapping }}
        PENTEST_APPLY_GUARDRAILS: ${{ inputs.pentest-apply-guardrails }}
        PENTEST_SYSTEM_PROMPT_ENABLED: ${{ inputs.pentest-system-prompt-enabled }}
        PENTEST_SYSTEM_PROMPT_TEXT: ${{ inputs.pentest-system-prompt-text }}
        PENTEST_CLEANUP_SYSTEM_PROMPT: ${{ inputs.pentest-cleanup-system-prompt }}
        PENTEST_DATASET_ENABLED: ${{ inputs.pentest-dataset-enabled }}
        PENTEST_DATASET_ID: ${{ inputs.pentest-dataset-id }}
        PENTEST_DATASET_NAME: ${{ inputs.pentest-dataset-name }}
        PENTEST_CLEANUP_DATASET: ${{ inputs.pentest-cleanup-dataset }}
        PENTEST_RESOURCE_SYSTEM_DESCRIPTION_ENABLED: ${{ inputs.pentest-resource-system-description-enabled }}
        PENTEST_RESOURCE_SYSTEM_DESCRIPTION_TEXT: ${{ inputs.pentest-resource-system-description-text }}
        PENTEST_CLEANUP_RESOURCE_SYSTEM_DESCRIPTION: ${{ inputs.pentest-cleanup-resource-system-description }}

        # Model Scanning
        MODEL_SCAN_POLICIES: ${{ inputs.model-scan-policies }}
        MODEL_SCAN_DESCRIPTION: ${{ inputs.model-scan-description }}

        # HuggingFace Onboarding
        HUGGINGFACE_ONBOARDING_ENABLED: ${{ inputs.huggingface-onboarding-enabled }}
        HUGGINGFACE_MODELS_TO_ONBOARD: ${{ inputs.huggingface-models-to-onboard }}
        HUGGINGFACE_ONBOARDING_PROJECT_ID: ${{ inputs.huggingface-onboarding-project-id }}
        HUGGINGFACE_ONBOARDING_PROJECT_NAME: ${{ inputs.huggingface-onboarding-project-name }}
        HUGGINGFACE_ONBOARDING_WAIT_SECS: ${{ inputs.huggingface-onboarding-wait-secs }}
        HUGGINGFACE_ONBOARDING_ONLY: ${{ inputs.huggingface-onboarding-only }}

        # Failure thresholds
        FAIL_OUTCOME_AT_OR_ABOVE: ${{ inputs.fail-outcome-at-or-above }}
        ON_THRESHOLD_ACTION: ${{ inputs.on-threshold-action }}
        ON_HARD_FAILURES_ACTION: ${{ inputs.on-hard-failures-action }}

        # GitHub Issues
        GITHUB_TOKEN: ${{ inputs.github-token || github.token }}
        GITHUB_REPOSITORY: ${{ inputs.github-repository != '' && inputs.github-repository || github.repository }}
        GITHUB_DEFAULT_LABELS: ${{ inputs.github-default-labels }}
        GITHUB_ASSIGNEES: ${{ inputs.github-assignees }}
        CATEGORY_ISSUE_MIN_SEVERITY: ${{ inputs.category-issue-min-severity }}

        # Concurrency & Polling
        MAX_CONCURRENT_PENTESTS: ${{ inputs.max-concurrent-pentests }}
        START_STAGGER_SECS: ${{ inputs.start-stagger-secs }}
        MAX_START_RETRIES: ${{ inputs.max-start-retries }}
        START_RETRY_DELAY: ${{ inputs.start-retry-delay }}
        POLL_TIMEOUT_SECS: ${{ inputs.poll-timeout-secs }}
        POLL_TIMEOUT_ACTION: ${{ inputs.poll-timeout-action }}
        GRAPHQL_POLL_INTERVAL_SECS: ${{ inputs.graphql-poll-interval-secs }}
      run: |
        set -euo pipefail

        RESULTS_DIR="${GITHUB_WORKSPACE}/.alltrue-results"
        mkdir -p "$RESULTS_DIR"

        export PYTHONPATH="${{ github.action_path }}:${PYTHONPATH:-}"

        echo "Using RESULTS_DIR=$RESULTS_DIR"
        cd "$RESULTS_DIR"

        # Run the action's script from the workspace results directory
        python "${{ github.action_path }}/scripts/run_pentest.py"

    # NOTE: remove once stable.
    - name: Debug locate outputs
      if: always()
      shell: bash
      run: |
        set -euo pipefail
        echo "=== Workspace results dir ==="
        ls -la "${{ github.workspace }}/.alltrue-results" || true

    - name: Extract outcome for outputs
      id: extract-outcome
      if: always()
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        set -euo pipefail
        RESULTS_DIR="${GITHUB_WORKSPACE}/.alltrue-results"

        normalize_outcome () {
          # Normalize common casing variants to: Critical|Poor|Moderate|Good|Excellent|Unknown
          local x="${1:-Unknown}"
          case "${x,,}" in
            critical) echo "Critical" ;;
            poor) echo "Poor" ;;
            moderate) echo "Moderate" ;;
            good) echo "Good" ;;
            excellent) echo "Excellent" ;;
            unknown|"") echo "Unknown" ;;
            *) echo "Unknown" ;;
          esac
        }

        worst_from_json () {
          local file="$1"
          if [ ! -f "$file" ]; then
            echo "Unknown"
            return
          fi
          python - "$file" <<'PY'
        import json, sys

        file = sys.argv[1]
        data = json.load(open(file))

        # Accept either:
        # - list[{"outcome": "..."}]
        # - {"results": list[...]} or other dict wrappers
        if isinstance(data, dict):
            if "results" in data and isinstance(data["results"], list):
                rows = data["results"]
            else:
                rows = next((v for v in data.values() if isinstance(v, list)), [])
        elif isinstance(data, list):
            rows = data
        else:
            rows = []

        outcomes = []
        for r in rows:
            if isinstance(r, dict) and r.get("outcome"):
                outcomes.append(str(r["outcome"]))

        order = ["Critical", "Poor", "Moderate", "Good", "Excellent"]
        norm = {o.lower(): o for o in order}

        found = {norm.get(o.lower(), o) for o in outcomes}
        for lvl in order:
            if lvl in found:
                print(lvl)
                break
        else:
            print("Unknown")
        PY
        }

        PENTEST_OUTCOME_RAW="$(worst_from_json "${RESULTS_DIR}/pentest_results_summary.json")"
        MODEL_OUTCOME_RAW="$(worst_from_json "${RESULTS_DIR}/model_scan_results_summary.json")"

        PENTEST_OUTCOME="$(normalize_outcome "$PENTEST_OUTCOME_RAW")"
        MODEL_OUTCOME="$(normalize_outcome "$MODEL_OUTCOME_RAW")"

        OVERALL="Unknown"
        if [ "$PENTEST_OUTCOME" = "Critical" ] || [ "$MODEL_OUTCOME" = "Critical" ]; then
          OVERALL="Critical"
        elif [ "$PENTEST_OUTCOME" = "Poor" ] || [ "$MODEL_OUTCOME" = "Poor" ]; then
          OVERALL="Poor"
        elif [ "$PENTEST_OUTCOME" = "Moderate" ] || [ "$MODEL_OUTCOME" = "Moderate" ]; then
          OVERALL="Moderate"
        elif [ "$PENTEST_OUTCOME" = "Good" ] || [ "$MODEL_OUTCOME" = "Good" ]; then
          OVERALL="Good"
        elif [ "$PENTEST_OUTCOME" = "Excellent" ] || [ "$MODEL_OUTCOME" = "Excellent" ]; then
          OVERALL="Excellent"
        fi

        echo "Pentest worst outcome: $PENTEST_OUTCOME"
        echo "Model scan worst outcome: $MODEL_OUTCOME"
        echo "Overall worst outcome: $OVERALL"

        echo "pentest_outcome=$PENTEST_OUTCOME" >> "$GITHUB_OUTPUT"
        echo "model_outcome=$MODEL_OUTCOME" >> "$GITHUB_OUTPUT"
        echo "overall_outcome=$OVERALL" >> "$GITHUB_OUTPUT"

    - name: Decide statuses (overall + per-phase)
      id: decide
      if: always()
      shell: bash
      run: |
        set -euo pipefail

        PENTEST_WORST="${{ steps.extract-outcome.outputs.pentest_outcome }}"
        MODEL_WORST="${{ steps.extract-outcome.outputs.model_outcome }}"
        OVERALL_WORST="${{ steps.extract-outcome.outputs.overall_outcome }}"

        THRESHOLD="${{ inputs.fail-outcome-at-or-above }}"
        THRESHOLD_ACTION="${{ inputs.on-threshold-action }}"

        idx () {
          case "${1,,}" in
            critical) echo 0 ;;
            poor) echo 1 ;;
            moderate) echo 2 ;;
            good) echo 3 ;;
            excellent) echo 4 ;;
            unknown|"") echo 999 ;;
            *) echo 999 ;;
          esac
        }

        threshold_idx="$(idx "$THRESHOLD")"

        status_for_outcome () {
          local worst="$1"
          local worst_idx
          worst_idx="$(idx "$worst")"

          if [ "$THRESHOLD_ACTION" != "fail" ] && [ "$THRESHOLD_ACTION" != "both" ]; then
            echo "neutral"
            return
          fi

          if [ "$threshold_idx" -eq 999 ] || [ "$worst_idx" -eq 999 ]; then
            echo "neutral"
            return
          fi

          if [ "$worst_idx" -le "$threshold_idx" ]; then
            echo "failure"
          else
            echo "success"
          fi
        }

        llm_status="$(status_for_outcome "$PENTEST_WORST")"
        model_status="$(status_for_outcome "$MODEL_WORST")"
        overall_status="$(status_for_outcome "$OVERALL_WORST")"

        echo "LLM pentest status: $llm_status (worst=$PENTEST_WORST)"
        echo "Model scan status:  $model_status (worst=$MODEL_WORST)"
        echo "Overall status:     $overall_status (worst=$OVERALL_WORST)"

        echo "llm_pentest_status=$llm_status" >> "$GITHUB_OUTPUT"
        echo "model_scan_status=$model_status" >> "$GITHUB_OUTPUT"
        echo "overall_status=$overall_status" >> "$GITHUB_OUTPUT"

    - name: Upload with timestamp
      if: always()
      shell: bash
      run: |
        set -euo pipefail
        TIMESTAMP=$(date +%Y%m%d-%H%M%S)
        echo "timestamp=$TIMESTAMP" >> "$GITHUB_ENV"

    # Upload everything under .alltrue-results as a single artifact (avoids glob issues)
    - name: Upload AllTrue scan results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: alltrue-scan-results-${{ env.timestamp }}
        path: ${{ github.workspace }}/.alltrue-results/**/*
        if-no-files-found: warn
        retention-days: ${{ inputs.artifact-retention-days }}
        include-hidden-files: true

    - name: Check scan outcome and fail workflow if needed
      if: always()
      shell: bash
      run: |
        set -euo pipefail

        STATUS="${{ steps.decide.outputs.overall_status }}"
        STATUS="${STATUS:-success}"

        echo "Final security scan overall status: ${STATUS}"

        if [ "$STATUS" = "failure" ]; then
          echo "Security scan failed: outcome threshold breached (or configured actions)."
          exit 1
        fi

        echo "Security scan did not fail the workflow (status=${STATUS})."
        exit 0
